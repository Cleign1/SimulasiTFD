{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================================================================================\n",
    "PROBLEM A5\n",
    "Build and train a neural network model using the Sunspots.csv dataset.\n",
    "Use MAE as the metrics of your neural network model.\n",
    "We provided code for normalizing the data. Please do not change the code.\n",
    "Do not use lambda layers in your model.\n",
    "The dataset used in this problem is downloaded from kaggle.com/robervalt/sunspots\n",
    "Desired MAE < 0.15 on the normalized dataset.\n",
    "========================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
    "    return ds.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/sunspots.csv'\n",
    "urllib.request.urlretrieve(data_url, 'sunspots.csv')\n",
    "\n",
    "time_step = []\n",
    "sunspots = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sunspots.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "    # YOUR CODE HERE\n",
    "    sunspots.append( )\n",
    "    # YOUR CODE HERE\n",
    "    time_step.append(  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series=  # YOUR CODE HERE\n",
    "\n",
    "# Normalization Function. DO NOT CHANGE THIS CODE\n",
    "min=np.min(series)\n",
    "max=np.max(series)\n",
    "series -= min\n",
    "series /= max\n",
    "time=np.array(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "split_time=3000\n",
    "\n",
    "\n",
    "time_train=  # YOUR CODE HERE\n",
    "x_train=  # YOUR CODE HERE\n",
    "time_valid=  # YOUR CODE HERE\n",
    "x_valid=  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "window_size=30\n",
    "batch_size=32\n",
    "shuffle_buffer_size=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=windowed_dataset(x_train, window_size=window_size,\n",
    "                            batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.Sequential([\n",
    "    # YOUR CODE HERE.\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_A5.h5\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
